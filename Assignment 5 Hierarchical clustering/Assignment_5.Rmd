---
title: "Assignment_5"
author: "Krishna Kumar Tavva - 811283461"
date: "2023-04-15"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# call the libraries

```{r}
library(readr)
library(tidyverse)
library(caret)
library(ISLR)
library(dplyr)
library(factoextra)
library(stats)
library(cluster)
library(ggplot2)
library(knitr)
library(ggcorrplot)
library(e1071)
library(reshape2)
library(cowplot)
library(pander)
library(kernlab)
library(tidyr)
library(fastDummies)
library(FactoMineR)
```

# Loading the data

```{r}
cs <- read.csv("E:\\Fundamentals of Machine Learning\\Module 8\\Cereals.csv")
summary(cs)
row.names(cs) <- cs[,1] #changing column name of Cereals data set to row name
```

# Looking for null values & omitting null values

```{r}
any(is.na.data.frame(cs))

cs1 <- na.omit(cs) #Remove NA (missing) values

```

# Normalize the data and finding the optimal k value by Elbow chart & Silhouette method

```{r}
set.seed(1)
cs2 <- scale(cs1[,-c(1:3,13)])
head(cs2)
```

#Q1:(PartA):Using Euclidean distance to the normalized measurements

```{r}
distance_table <- get_dist(cs2) #Compute the distances. Euclidean distance is default.
fviz_dist(distance_table) #fviz_dist() function visualizes a distance matrix
```
#This graph is a distance matrix. As we can see, the diagonal values are zeros (dark orange) because it is showing the distance between any point against itself. The purple and blue represent the furthest distance between any pair of observations.

#Looking at the Correlation between Variables.

```{r}
corr <- cor(cs2)
ggcorrplot(corr, outline.color = "grey25", lab = TRUE, hc.order = FALSE, type = "full")
```
#Sugar and calories are highly negatively correlated with rating. Also, Potass is highly positively correlated with fiber and Protien.

#Trying to Understand the variable variance by performing principle component analysis

```{r}
pca_cereal <- PCA(cs2) #perform principal component analysis
```


```{r}
pca_cereal <- prcomp(cs2, scale = TRUE) #variable has mean zero and standard deviation one
loadings <- pca_cereal$rotation #extract loading
print(loadings[, 1:2])#print loading for the first two PCs
```


```{r}
var <- get_pca_var(pca_cereal)
fviz_pca_var(pca_cereal, col.var="contrib",
gradient.cols = c("yellow","red","blue","grey","purple"),
ggrepel = TRUE ) + labs( title = "PCA Variable Variance")
```

#From PCA Variable Variance, we can infer that Sugar , calories, protien potass and fiber contribute more in the two PCA components/dimensions (Variables)


```{r}
Elbow <- fviz_nbclust(cs2, kmeans, method="wss")
Elbow
silhouette <- fviz_nbclust(cs2,kmeans,method="silhouette")
silhouette
```

```{r}
set.seed(1)
k10 <- kmeans(cs2, centers = 10, nstart = 25) # k = 10, number of restarts = 25
k5 <- kmeans(cs2, centers = 5, nstart = 25) # k = 5, number of restarts = 25
```


```{r}
k10$centers
```


```{r}
k10$size
```

```{r}
fviz_cluster(k10, data = cs2)
```

#After applying both the silhouette method and elbow method, we obtained K value as 10, which we used to plot the 10 clusters. However, upon observing the plot, we noticed that some of the clusters were overlapping, indicating that using only K-means clustering may not be the best option for optimization. Therefore, we will apply hierarchical clustering to obtain an optimal number of clusters.

#Q1:(PartB) Apply hierarchical clustering. Use Agnes to compare the clustering from single linkage, completelinkage, average linkage, and Ward. Choose the best method.

```{r}
set.seed(1)
hierarchical_cluster <- hclust(distance_table, method = "complete") #hierarchical clustering using Complete Link
plot(hierarchical_cluster, cex = 0.6, hang = -1, main = "Dendrogram of Hierarchical Clustering") #Plot the obtained dendrogram
rect.hclust(hierarchical_cluster, k = 10, border = 2:10)
```

# Compute with agnes and with different linkage methods

```{r}
hc_single<-agnes(distance_table, method ="single")
hc_complete<-agnes(distance_table, method ="complete")
hc_average<-agnes(distance_table, method ="average")
hc_ward <- agnes(distance_table, method = "ward")
```
#Compare Agglomerative coefficients

```{r}
print(hc_single$ac)
print(hc_complete$ac)
print(hc_average$ac)
print(hc_ward$ac)
```
#After comparing the Agglomerative coefficients the best linakage method is ward linkage i.e. 0.90 accuracy

#Q2: How many clusters would you choose?

```{r}
#Utilizing the Ward linkage, 5 clusters seem to be a good number to group the data
set.seed(1)
fviz_dend(hc_ward, k = 5,main = "Dendrogram of AGNES (Ward)",
cex = 0.5, k_colors = c("black", "purple", "darkgreen", "darkorange", "darkred"), 
color_labels_by_k = TRUE,labels_track_height = 16,ggtheme = theme_bw()) #Plot the Dendrogram of AGNES
```

```{r}
cs2_5 <- cutree(hc_ward, k = 5)
Clustered_df <-as.data.frame(cbind (cs2, cs2_5 ))
```

#Q3:Comment on the structure of the clusters and their stability. Hint: To check stability, partition the data, and see how well clusters formed based on one part apply to the other part. 
#Q3: PartA: Cluster partition A

```{r}
#We will partition the dataset into two groups: Training A and Validation B.
set.seed(1) #To get the same random variables
TrainingA <- cs2[1:55,]
nrow(TrainingA)
```
```{r}
ValidationB <- cs2[56:74,]
nrow(ValidationB)
```
# Compute the distances. Euclidean distance is used by default. Looking at the cluster of trainingA and ValidationB data set. 

```{r}
set.seed(1) # To maintain same values
distance_TrainA <- get_dist(TrainingA)
# Compute with AGNES and with different linkage methods For Training Dataset
hc_single_TrainA <- agnes(distance_TrainA, method = "single")
hc_complete_TrainA <- agnes(distance_TrainA, method = "complete")
hc_average_TrainA <- agnes(distance_TrainA, method = "average")
hc_ward_TrainA <- agnes(distance_TrainA, method = "ward")
print(hc_single_TrainA$ac)
print(hc_complete_TrainA$ac)
print(hc_average_TrainA$ac)
print(hc_ward_TrainA$ac)
```
#It allows us to determine that the best linkage is Ward with 88.91% accuracy for validationA

## Compute with AGNES and with different linkage methods For Training Dataset

```{r}
set.seed(1) # To maintain same values
distance_ValidB <- get_dist(ValidationB)

# Compare AGNES (agglomerative) coefficients
hc_single_ValidB <- agnes(distance_ValidB, method = "single")
hc_complete_ValidB <- agnes(distance_ValidB, method = "complete")
hc_average_ValidB <- agnes(distance_ValidB, method = "average")
hc_ward_ValidB <- agnes(distance_ValidB, method = "ward")
print(hc_single_ValidB$ac)
print(hc_complete_ValidB$ac)
print(hc_average_ValidB$ac)
print(hc_ward_ValidB$ac)
```
#It allows us to determine that the best linkage is Ward with 77.10% accuracy for validationB

#Dendrogram for TrainingA and ValidationB dataset 

```{r}
fviz_dend(hc_ward_TrainA, k = 5,main = "Training A -Dendrogram of AGNES",
cex = 0.5, k_colors = c("black", "purple", "darkgreen", "darkorange", "darkred"),
color_labels_by_k = TRUE,labels_track_height = 16,ggtheme = theme_bw()) #Plot the Dendrogram of AGNES

fviz_dend(hc_ward_ValidB, k = 5,main = "Validation B- Dendrogram of AGNES",
cex = 0.5, k_colors = c("black", "purple", "darkgreen", "darkorange", "darkred"), 
color_labels_by_k = TRUE,labels_track_height = 16,ggtheme = theme_bw()) #Plot the Dendrogram of AGNES
```

#Q3:PartB: Method1 Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid)

```{r}
Clustered_df_A <-cutree (hc_ward_TrainA, k=5)
Clusters_A <-as.data.frame(cbind(TrainingA, Clustered_df_A))
nrow(Clusters_A)#55

Clust_1 <- colMeans (Clusters_A [Clusters_A$ Clustered_df_A == "1" ,]) #This results in a vector of mean values for each column of the data, which represents the centroid of cluster 1 
Clustered_df_B <-cutree (hc_ward_ValidB, k=5)
Clusters_B <-as.data.frame(cbind(ValidationB, Clustered_df_B))
nrow(Clusters_B)#55

Clust_2 <- colMeans (Clusters_B [Clusters_B$ Clustered_df_B == "1" ,]) #This results in a vector of mean values for each column of the data, which represents the centroid of cluster 2 
Centroid <-rbind(Clust_1, Clust_2)
Centroid 

```

#On overall level the both the cluster seems fine but also a slight difference is - 
#Cluster_1 has a higher fiber and potassium content compared to Cluster_2, which may suggest that cereals in this cluster are healthier or more nutrient-dense.
#Cluster_2 has a higher sugar content compared to Cluster_1, which may suggest that cereals in this cluster are less healthy or have more added sugars.

#Q3:PartB: Method2 Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid)

#In order to predict the calculate distances between each record in data set B and the cluster centroids

```{r}
distances <- dist(ValidationB[, -1], TrainingA, method = "euclidean") #This line calculates the pairwise distances between the validation and the training samples, using the Euclidean distance metric.
hc <- hclust(distances) #This line performs hierarchical clustering on the distances object, using the default "complete" linkage method
clusterB <- cutree(hc, k = 5) #This line cuts the hierarchical tree into five clusters based on the hc object, using the cutree()
ValidationB$cluster <- clusterB #This line adds a new column to the ValidationB data frame called "cluster"
ValidationB$cluster
```

#The predicted clusters of B on the basis of centroids of A almost classified same except 3 cereals which are "special_K", "Total_CF" and "Total_WG". Out of 19 only 3 observation changed their cluster after comparing the validation data set with Training dataset.It means the stability of clusters are really high.

#Q3:PartC: Assess how consistent the cluster assignments are compared to the assignments based on all the data

#Method 1: We are comparing the mean values of each feature for the two clusters identified in the two datasets. These centroids can be used to compare the features of the two clusters and explore differences or similarities between them.Here we can see that Cluster_1 has a higher fiber and potassium content compared to Cluster_2, which may suggest that cereals in this cluster are healthier or more nutrient-dense.Cluster_2 has a higher sugar content compared to Cluster_1, which may suggest that cereals in this cluster are less healthy or have more added sugars hence cluster 2 rating is really low compared to cluster 1.

#Method 2:This method calculates the pairwise Euclidean distances between the records in the ValidationB dataset and the cluster centroids obtained from the TrainingA dataset using hierarchical clustering with complete linkage method.This enables the prediction of the cluster labels for the validation dataset using the centroids obtained from the training dataset. hence we can see the stability of validation data set on the basis of training dataset. We can see the cereals are cluster exactly the same except "special_K", "Total_CF" and "Total_WG". Out of 19 only 3 observation changed their cluster after comparing the validation data set with Training dataset.It means the stability of clusters are really high.


#Q4:The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.”Should the data be normalized? If not, how should they be used in the cluster analysis?

#To analyze which group of cereals are healthier to distribute daily in cafeterias in elementary public schools,we will use the non-standardized dataset. In my opinion, it is more meaningful and easier to compare if we look at the variables in their original scale.Here is a table summarizing the number of cereals per cluster:

```{r}
Healthy_data <-as.data.frame(cbind (cs1, cs2_5 ))
Healthy_data_sort <- Healthy_data[order(Healthy_data$cs2_5),c(1,17)]
Count_cluster <- Healthy_data_sort %>% group_by(cs2_5) %>% summarise(count = n())
print(Count_cluster)

#Summary table showing the median of each variable
Healthy_data_Var <- Healthy_data [,4:17]
cluster_table <- Healthy_data_Var %>% group_by(cs2_5) %>% 
summarize(across(.cols = everything(), .fns = median))
print(cluster_table)
```

# Create bar graph

```{r}
calories <- ggplot(cluster_table, aes(x = cs2_5, y = calories)) + 
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Cluster", y = "Calories") +
  ggtitle("Cluster by Calories")

protein <- ggplot(cluster_table, aes(x = cs2_5, y = protein)) + 
  geom_bar(stat = "identity", fill = "red") +
  labs(x = "Cluster", y = "protein") +
  ggtitle("Cluster by Protein")

fat <- ggplot(cluster_table, aes(x = cs2_5, y = fat)) + 
  geom_bar(stat = "identity", fill = "orange") +
  labs(x = "Cluster", y = "fat") +
  ggtitle("Cluster by Fat")

sodium <- ggplot(cluster_table, aes(x = cs2_5, y = sodium)) + 
  geom_bar(stat = "identity", fill = "pink") +
  labs(x = "Cluster", y = "sodium") +
  ggtitle("Cluster by sodium")

fiber <- ggplot(cluster_table, aes(x = cs2_5, y = fiber)) + 
  geom_bar(stat = "identity", fill = "gray") +
  labs(x = "Cluster", y = "fiber") +
  ggtitle("Cluster by fiber")

carbo <- ggplot(cluster_table, aes(x = cs2_5, y = carbo)) + 
  geom_bar(stat = "identity", fill = "brown") +
  labs(x = "Cluster", y = "carbo") +
  ggtitle("Cluster by carbo")

sugars <- ggplot(cluster_table, aes(x = cs2_5, y = sugars)) + 
  geom_bar(stat = "identity", fill = "lightgreen") +
  labs(x = "Cluster", y = "sugars") +
  ggtitle("Cluster by sugars")

potass <- ggplot(cluster_table, aes(x = cs2_5, y = potass)) + 
  geom_bar(stat = "identity", fill = "yellow") +
  labs(x = "Cluster", y = "potass") +
  ggtitle("Cluster by potass")

rating <- ggplot(cluster_table, aes(x = cs2_5, y = rating)) + 
  geom_bar(stat = "identity", fill = "black") +
  labs(x = "Cluster", y = "rating") +
  ggtitle("Cluster by rating")

plot_grid(calories, protein, fat, sodium, fiber, carbo, sugars, potass, rating)
```

#Based on the graphs, we can see that Cluster 1 has the lowest values for calories, fat, and sugars and the highest values for protein, fiber, and vitamins, which suggests that it may contain cereals that are generally considered healthier options and thats why it has very high rating as well. That why Cluster 1 fits the needs of our client! Nevertheless, part of our client’s petition is to have a different cereal per day, which this cluster does not satisfy this need. For this reason, we will also recommend cluster 5 to satisfy this request. Cluster 5 has zero fats, Zero sugars, and it has the second-lowest number of calories after cluster 1. It also has a good number of proteins and fiber.On the other hand, Cluster 3 has the highest values for calories and sugars and the lowest values for protein, fiber, and vitamins, which suggests that it may contain cereals that are generally considered less healthy. we saw the same insight from our correlation plot high sugar less rating because its less healthy.  However, it's important to note that this is just a general observation and individual cereals within each cluster may vary in terms of their nutritional value.
